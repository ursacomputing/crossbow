---
title: "Arrow Release Benchmark Report"
execute: 
  warning: false
  echo: false
format: 
  html:
    grid:
        sidebar-width: 0px
        body-width: 2000px
        margin-width: 340px
        gutter-width: 1.5rem
    self-contained: true
    page-layout: full
    toc: true
    toc-title: "Contents"
    toc-depth: 3
    margin-left: 30px
    link-external-newwindow: true
    theme: cosmo
---


```{r setup}
#| include: false

Sys.unsetenv("CONBENCH_PASSWORD")


baseline_git_commit <- Sys.getenv("BASELINE_GIT_COMMIT") 
contender_git_commit <-  Sys.getenv("CONTENDER_GIT_COMMIT")
# baseline_git_commit <- '2c5ae51a17bf8e7f63b393e89e2aeb0c2b75b1b6'
# contender_git_commit <-  'e0708c7d0ff40bd8d89aefbac5b358c94affe871'
hardware_name  <- c("amd64-m5-4xlarge-linux", "amd64-c6a-4xlarge-linux")

library(dplyr)
library(ggplot2)
library(tidyr)
library(glue)
library(purrr)
library(conbenchcoms)
library(cli)
library(lubridate)
library(knitr)
library(gt)
library(forcats)
library(showtext)
library(ggiraph)
library(janitor)

## add fonts for ggplot2
## TODO: add for tooltip font
font_add_google('Roboto Mono', 'roboto')
showtext_auto()

fig_width <- 16
fig_height <- 7

## tweaks to theme
theme_set(theme_minimal(base_size = 24, base_family = "roboto") %+replace%
  theme(
    plot.title.position = "plot",
    strip.text.y = element_text(angle = -90),
    strip.text = element_text(size = 18),
    panel.border = element_rect(colour = "grey80", fill = NA, linewidth = 1),
    legend.title = element_blank(),
    legend.position = "top"
  ))
  
opts_chunk$set(
  cache = FALSE,
  echo = FALSE
  )

baseline_git_commit_short <- substr(baseline_git_commit, 1, 7)
contender_git_commit_short <- substr(contender_git_commit, 1, 7)

source("R/functions.R")
```

```{r warn-gha}
if (!is_gha()) {
  message("When run locally, this report will cache results to avoid re-running benchmarks. 
To see the most up-to-date results, please run this report on GitHub Actions or delete the cache.")
}
```


```{r warn-commits-not-set}
if (!nzchar(baseline_git_commit) | !nzchar(contender_git_commit)) {
  knit_exit("Please set the BASELINE_GIT_COMMIT & CONTENDER_GIT_COMMIT environment variables to the commit hash of the baseline and the release.")
}
```

```{r compute-conbench-requests}
#| include: false
#| results: 'asis'
#| cache: !expr '!is_gha()'

run_comp <- find_runs(baseline_git_commit, contender_git_commit, hardware_name) |>
    filter(id != "5200ba71e40e462da1cdbb7ff57fcc50") ## old m5 that we don't want for comparisons
  

if (length(run_comp) == 0) {
  knit_exit("No runs found for the given commits. Please check that the commits are correct and that the benchmark runs have completed.")
}

# Compare the baseline to the contender for 
# macrobenchmarks
m5_bm <- run_comp %>%
  filter(hardware.name == "amd64-m5-4xlarge-linux") %>%
  compare_baseline_to_contender()

macro_bm_df <- m5_bm %>%
  filter(baseline.language %in% c("Python", "R")) 

# microbenchmarks
micro_bm_df <- run_comp %>%
  filter(hardware.name == "amd64-c6a-4xlarge-linux") %>%
  compare_baseline_to_contender() %>% 
  filter(baseline.language %in% c("C++", "JavaScript", "Java")) |> 
  bind_rows(m5_bm %>% filter(baseline.language %in% "JavaScript"))
```


```{r message-missing-hardware}
#| results: 'asis'

## extract hardware information
hardware_summary <- run_comp %>%
  mutate(commit.timestamp = ymd_hms(commit.timestamp)) %>%
  mutate(links.self = glue("https://conbench.ursa.dev/runs/{id}")) %>% 
  distinct(run_type, links.self, commit.sha, commit.timestamp, commit.url, hardware.cpu_model_name, id)

if (nrow(hardware_summary) != 2L*length(hardware_name)) {
  ## fail early and informatively with when not using correct hardware
  missing_commits <- setdiff(c(contender_git_commit, baseline_git_commit), unique(hardware_summary$commit.sha)[1])
  cat(pluralize("Commits {missing_commits} w{?as/ere} not benchmarked with {glue_collapse(hardware_name, last = ' & ')} hardware."))
  knit_exit()
}
```



# Benchmark Run Summary
```{r table-run-summary}

## get additional information about the benchmarks
benchmark_summary <- micro_bm_df %>%
  bind_rows(macro_bm_df) %>%
  select(starts_with("baseline") | starts_with("contender")) %>%
  pivot_longer(
    cols = everything(),  # Select all columns
    names_to = c("run_type", ".value"),  # Define the columns for group and values
    names_pattern = "(.*?)\\.(.*)"  # Use regex to split column names into group and value
  ) %>% 
  get_language_summary_from_comparison() 


run_summary <- hardware_summary %>%
  left_join( ## join on run_id
    benchmark_summary,
    by = c("id" = "run_id")
  ) %>%
  mutate(
    commit.sha = glue("[{substr(commit.sha, 1, 7)}]({commit.url})"),
    run_type = glue("[{run_type}]({links.self})"),
    commit.timestamp = ymd_hms(commit.timestamp)
  ) %>%
  select(run_type, commit.sha, commit.timestamp, hardware.cpu_model_name, languages, benchmark_type, n_benchmarks) %>%
  arrange(benchmark_type, hardware.cpu_model_name)

run_summary %>% 
  gt() %>% 
  fmt_markdown(c("commit.sha", "run_type", "benchmark_type")) %>% 
  gt::cols_label(
    run_type = "Run Type",
    commit.sha = "Commit SHA",
    commit.timestamp = "Time of Commit",
    hardware.cpu_model_name = "Hardware",
    languages = "Languages",
    benchmark_type = "Benchmark Type",
    n_benchmarks = "Number of Benchmarks"
  ) %>% 
  tab_footnote(
    "When we compare benchmark results, we always have a contender (the new code that we are considering) and a baseline (the old code that were are comparing to). The historic distribution will be drawn from all benchmark results on commits in the baseline commit's git ancestry, up to and including all runs on the baseline commit itself. In this context, a baseline is typically the last Arrow release and the contender is the current release candidate.",
    locations = cells_column_labels(columns = "run_type")
  ) %>% 
  opt_table_font(font = google_font("Roboto Mono"))
```

```{r message-same-day-commit}
#| results: 'asis'

if (Sys.Date() %in% unique(as.Date(hardware_summary$commit.timestamp))) {
  ## inform early and informatively with when not using correct hardware
  cat("::: {.callout-warning}\n")
  cat("You are trying to examine commits that were made today. It is possible that not all benchmarks have been completed yet and plots below may show unexpected results.\n")
  cat(":::\n")
}
```

# Macrobenchmarks {#macro-bm}



```{r function-plots}
#| include: false

change_cols <- viridisLite::mako(3, begin = 0.5, end = 0.75)
names(change_cols) <- c(glue("{contender_git_commit_short} (contender) faster"), glue("{baseline_git_commit_short} (baseline) faster"))
```


```{r compute-process-macro-bm-data}
#| results: 'asis'

macro_bm_proced <- macro_bm_df %>%
  # csv-read had very different kinds of runs
  filter(baseline.benchmark_name != "csv-read") %>%
  # Remove the "TPCH-" prefix and any 0 at the start of the number
  mutate(baseline.tags.query_id = gsub("^0", "", gsub("TPCH-", "", baseline.tags.query_id))) %>%
  mutate(dataset = case_when(
    baseline.benchmark_name == "tpch" ~ glue("Query id: {baseline.tags.query_id}"),
    baseline.benchmark_name == "wide-dataframe" ~ "wide-dataframe",
    baseline.benchmark_name == "partitioned-dataset-filter" ~ glue("{baseline.benchmark_name}: {baseline.tags.query} query"),
    TRUE ~ baseline.tags.dataset
  )) %>%
  group_by(baseline.language, baseline.benchmark_name) %>%
  group_map(~ tidy_compare(.x, .y))

macro_bm_list <- list()
for (i in seq_len(length(macro_bm_proced))) {
  one_conbench <- macro_bm_proced[[i]]
  lang <- unique(one_conbench$language)
  benchmark_name <- unique(one_conbench$benchmark_name)
  macro_bm_list[[lang]][[benchmark_name]] <- one_conbench
}

tpch_data <- macro_bm_list[["R"]][["tpch"]]

macro_bm_list[["R"]][which(names(macro_bm_list[["R"]]) %in% c("tpch"))] <- NULL
```

Live Conbench UI views for the macrobenchmarks are available at this [url](`r generate_compare_url(macro_bm_df)`). Conbench is an additional method to explore the results of the benchmarks particularly if you want to see results from more of the history or see more metadata. 

## Benchmark Percent Changes

- Benchmarks are plotted using the percent change from baseline to contender. 
- Additional information on each benchmark is available by hovering over the relevant bar.

## Python
```{r plot-python}
#| echo: FALSE
#| results: 'asis'
#| warning: FALSE
 
#names(macro_bm_list)
lang <- "Python"

bm_names <- names(macro_bm_list[[lang]])
bm_names <- bm_names[!bm_names %in% c("dataset-serialize")] ## dropping dataset-serialize for now

python_plots <- map_chr(bm_names, \(bm_name) {
  knit_child(
    text = c(
      glue("```{r plot-<<lang>>-<<bm_name>>}", .open = "<<", .close = ">>"),
      "#| results: 'asis'",
      'cat("###", bm_name, "\n")',
      "plot_comparison(macro_bm_list[[lang]][[bm_name]])",
      "```"
    ),
    envir = environment(),
    quiet = TRUE
  )
})

cat(python_plots, sep = "\n")
```


## R
```{r plot-r}
#| echo: FALSE
#| results: 'asis'
#| warning: FALSE
 
lang <- "R"
bm_names <- names(macro_bm_list[[lang]])

r_plots <- map_chr(names(macro_bm_list[[lang]]), \(bm_name) {
  knit_child(
    text = c(
      glue("```{r plot-<<lang>>-<<bm_name>>}", .open = "<<", .close = ">>"),
      "#| results: 'asis'",
      'cat("###", bm_name, "\n")',
      "plot_comparison(macro_bm_list[[lang]][[bm_name]])",
      "```"
    ),
    envir = environment(),
    quiet = TRUE
  )
})

cat(r_plots, sep = "\n")
```

### tpch 
```{r plot-tpch}
#| echo: FALSE
#| results: 'asis'
#| warning: FALSE

tpch_p <- tpch_data %>%
  mutate(change_lab = change * 1.05) %>%
  mutate(query_id = as.numeric(query_id)) %>%
  ggplot() +
  aes(y = fct_reorder(dataset, query_id, .desc = TRUE), x = change, fill = pn_lab) +
  geom_col_interactive(
    aes(
      tooltip = glue(
        "Percent change: {round(change, 2)}%\n",
        "Difference: {difference}\n",
        "Dataset: {dataset}"
      )
    ),
    position = "dodge", alpha = 0.75
  ) +
  geom_text(aes(x = change_lab, hjust = ifelse(change >= 0, 0, 1), label = difference, colour = pn_lab), size = 7) +
  geom_vline(xintercept = 0, colour = "grey80") +
  facet_grid(rows = vars(format), cols = vars(scale_factor), labeller = label_both) +
  scale_fill_manual(values = change_cols) +
  scale_colour_manual(values = change_cols) +
  scale_x_continuous(expand = expansion(mult = 0.6)) +
  guides(colour = "none") +
  ylab("Dataset") +
  xlab("% change")

girafe(
  ggobj = tpch_p,
  options = list(
    opts_tooltip(use_fill = TRUE),
    opts_sizing(width = .7)
  ),
  width_svg = 16,
  height_svg = 20
)
```


# Microbenchmarks {#micro-bm}

```{r compute-process-micro-bm-data}


## unlike the plots above, this is added as single rectangular
## data.frame because that data structure is easier to work with
## using arquero
micro_bm_proced <- micro_bm_df %>%
  filter(!is.na(baseline.language)) %>%
  group_by(baseline.language, baseline.benchmark_name) %>% 
  group_modify(~ tidy_compare(.x, .y)) %>% 
  ungroup() %>% 
  filter(!is.na(name)) %>% 
  # filter(!is.na(analysis.lookback_z_score.regression_indicated)) %>% ## indicator of some empty data
  ## this will enable the yaxis to be populated with names when params is NA. params is preferable because it is more specific
  mutate(params = ifelse(is.na(params), baseline.case_permutation, params)) %>% 
  rowwise() %>%
  mutate(params = paste(strwrap(params, 10), collapse="\n")) %>% 
  clean_names() %>% 
  select(language, baseline_benchmark_name, name, params, suite, analysis_pairwise_improvement_indicated, analysis_pairwise_regression_indicated, change, difference, pn_lab, analysis_pairwise_percent_change, baseline_single_value_summary, contender_single_value_summary, cb_url, unit)
```

There are currently `r nrow(micro_bm_proced)` microbenchmarks in the Arrow benchmarks. The following comparisons are also available to be viewed in the [Conbench UI](`r generate_compare_url(micro_bm_df)`). 

```{r table-micro-bm-summary}
micro_bm_proced %>% 
  count(language, analysis_pairwise_regression_indicated, analysis_pairwise_improvement_indicated) %>% 
  mutate(col_var = case_when(
    analysis_pairwise_regression_indicated == TRUE ~ "Regressions",
    analysis_pairwise_improvement_indicated == TRUE ~ "Improvements",
    is.na(analysis_pairwise_regression_indicated) | is.na(analysis_pairwise_improvement_indicated) ~ "No comparison",
    TRUE ~ "Stable"
  )) %>%
  select(-all_of(starts_with("analysis_pairwise"))) %>% 
  pivot_wider(names_from = col_var, values_from = n) %>% 
  rowwise() %>%
  mutate(Total = sum(c_across(c(Stable, Improvements, Regressions)), na.rm = TRUE)) %>%
  gt() %>% 
  cols_label(language = "Language") %>% 
  tab_spanner(
    label = "Number of microbenchmarks",
    columns = c(Stable, Improvements, Regressions, `No comparison`, Total)
  ) %>%
  opt_table_font(font = google_font("Roboto Mono"))
```

Because of the large number of benchmarks, the top 20 benchmark results that deviate most from the baseline in both the positive and negative directions are presented below. _All_ microbenchmark results for this comparison can be explored interactively in the [microbenchmark explorer](#micro-bm-explorer).

::: {.callout-important collapse="true"}

## Largest 20 regressions between baseline and contender

```{r table-top-zscores-negative}
top_perf_table(micro_bm_proced, direction = "regression")
```


:::

::: {.callout-note collapse="true"}

## Largest 20 improvements between baseline and contender

```{r table-top-zscores-positive}
top_perf_table(micro_bm_proced, direction = "improvement")
```

:::


```{r ojs-defn}
ojs_define(ojs_micro_bm_proced = micro_bm_proced)
ojs_define(ojs_change_cols = rev(change_cols))
ojs_define(ojs_pn_lab = unique(micro_bm_proced$pn_lab))
```

```{ojs setup-micro-bm}
import { aq, op } from '@uwdata/arquero';
boxWidth = 900
microBmProced = aq.from(transpose(ojs_micro_bm_proced))
```

## Microbenchmark explorer {#micro-bm-explorer}

This microbenchmarks explorer allows you to filter the microbenchmark results by language, suite, and benchmark name and toggle regressions and improvements based on a percent change between the baseline and contender |> . Languages, suite and benchmark name need to be selected to show a benchmark plot. Additional benchmark parameters are displayed on the vertical axis resulting in each bar representing a case permutation. If a benchmark does not have additional parameters, the full case permutation string is displayed. Each bar can be clicked to open the Conbench UI page for that benchmark providing additional history and metadata for that case permutation.

```{ojs filter-micro-bm}
// Top level: are there regressions/improvements?
viewof changes = Inputs.checkbox(["Regressions", "Improvements"], {
  label: md`**Benchmark Status**`,
  value: ["Regressions"]
  })

// Choose the state of the benchmark
microBmProcedChanges = {
  let microBmProcedParams;
  let hasRegressions = changes.includes("Regressions");
  let hasImprovements = changes.includes("Improvements");
  microBmProcedParams = microBmProced
      .params({hr: hasRegressions, hi: hasImprovements})
  if (hasRegressions && hasImprovements) {
    microBmProcedParams = microBmProced
      .filter((d, $) => d.analysis_pairwise_regression_indicated==$.hr || d.analysis_pairwise_improvement_indicated==$.hi);
  } else if (hasImprovements) {
    microBmProcedParams = microBmProced
      .filter((d, $) => d.analysis_pairwise_improvement_indicated==$.hi)
  } else if (hasRegressions) {
    microBmProcedParams = microBmProced
      .filter((d, $) => d.analysis_pairwise_regression_indicated==$.hr);
  } else {
    microBmProcedParams = microBmProced;
  }
  return microBmProcedParams;
}

// Choose the language
allLanguageValues = [null].concat(microBmProcedChanges.dedupe('language').array('language'))

viewof languageSelected = Inputs.select(allLanguageValues, {
    label: md`**Language**`,
    value: [allLanguageValues[0]],
    width: boxWidth
})

languages = {
  return (languageSelected === null)
  ? microBmProcedChanges // If languageSelected is "All languages", no filtering is applied
  : microBmProcedChanges.filter(aq.escape(d => op.includes(d.language, languageSelected)));
}


allSuiteValues = [null].concat(languages.dedupe('suite').array('suite'))

// Choose the suite
viewof suiteSelected = Inputs.select(allSuiteValues, {
    label: md`**Suite**`,
    value: [allSuiteValues[0]],
    width: boxWidth
})


suites = {
  return (suiteSelected === null)
  ? languages 
  : languages.filter(aq.escape(d => op.includes(d.suite, suiteSelected)));
}

allNameValues = [null].concat(suites.dedupe('name').array('name'))

// Choose the benchmark
viewof nameSelected = Inputs.select(allNameValues, {
    label: md`**Benchmark Name**`,
    value: [allNameValues[0]],
    width: boxWidth
})

microBmProcedChangesFiltered = {
  return (nameSelected === null)
  ? suites 
  : suites.filter(aq.escape(d => op.includes(d.name, nameSelected)));
}
```

```{ojs plot-micro-bm}
// long labels need special handlings
margins = {
  let hasRegressions = changes.includes("Regressions");
  let hasImprovements = changes.includes("Improvements");
  let margin = [300, 300];
  if (hasRegressions && hasImprovements) {
    margin = margin;
  } else if (hasImprovements) {
    margin = [0, 600];
  } else if (hasRegressions) {
    margin = [600, 0];
  } 
  return margin;
}

displayPlot = nameSelected !== null && suiteSelected !== null && languageSelected !== null

// Only display plots if a benchmark is selected
mbPlot = {
  if (displayPlot) {
    return Plot.plot({
      width: 1200,
      height: microBmProcedChangesFiltered.numRows() * 30 + 100, //adjust height of plot based on number of rows
      marginRight: margins[0],
      marginLeft: margins[1],
      label: null,
      x: {
        axis: "top",
        label: "% change",
        labelAnchor: "center",
        labelOffset: 30
      },
      style: {
        fontSize: "14px",
        fontFamily: "Roboto Mono"
      },
      color: {
        range: ojs_change_cols,
        domain: ojs_pn_lab,
        type: "categorical",
        legend: true
      },
      marks: [
        Plot.barX(microBmProcedChangesFiltered, {
          y: "params",
          x: "change",
          color: "black",
          fill: "pn_lab",
          fillOpacity: 0.75,
          sort: { y: "x" },
          channels: { difference: "difference", params: "params" },
          href: "cb_url",
          tip: true
        }),
        Plot.gridX({ stroke: "white", strokeOpacity: 0.5 }),
        Plot.ruleX([0]),
        d3
          .groups(microBmProcedChangesFiltered, (d) => d.change > 0)
          .map(([posneg, dat]) => [
            Plot.axisY({
              x: 0,
              ticks: dat.map((d) => d.params),
              tickSize: 0,
              anchor: posneg ? "left" : "right"
            })
          ])
      ]
    });
  } else {
    return md`**Language, suite and benchmark all need a selection for a plot to be displayed.**`;
  }
}
```


```{css, echo=FALSE}
div.main-container {
  max-width: 2000px;
}
```

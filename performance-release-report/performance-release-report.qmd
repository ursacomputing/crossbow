---
title: "Arrow Release Benchmark Report"
execute: 
  warning: false
  echo: false
format: 
  html:
    grid:
        sidebar-width: 0px
        body-width: 2000px
        margin-width: 340px
        gutter-width: 1.5rem
    self-contained: true
    page-layout: full
    toc: true
    toc-title: "Contents"
    toc-depth: 3
    margin-left: 30px
    link-external-newwindow: true
    theme: cosmo
---


```{r setup}
#| include: false

Sys.unsetenv("CONBENCH_PASSWORD")

# baseline_git_commit <- Sys.getenv("BASELINE_GIT_COMMIT", '5bf86ab4d9e9bc5bb7e1c6e65a55d9f1723597bf') 
# contender_git_commit <-  Sys.getenv("CONTENDER_GIT_COMMIT", 'b7d2f7ffca66c868bd2fce5b3749c6caa002a7f0')
baseline_git_commit <- '5bf86ab4d9e9bc5bb7e1c6e65a55d9f1723597bf'
contender_git_commit <-  'b7d2f7ffca66c868bd2fce5b3749c6caa002a7f0'
hardware_name <- c("ursa-i9-9960x", "ursa-thinkcentre-m75q")

library(dplyr)
library(ggplot2)
library(tidyr)
library(glue)
library(purrr)
library(conbenchcoms)
library(cli)
library(lubridate)
library(knitr)
library(gt)
library(forcats)
library(showtext)
library(ggiraph)
library(janitor)

## add fonts for ggplot2
## TODO: add for tooltip font
font_add_google('Roboto Mono', 'roboto')
showtext_auto()

fig_width <- 16
fig_height <- 7

## tweaks to theme
theme_set(theme_minimal(base_size = 24, base_family = "roboto") %+replace%
  theme(
    plot.title.position = "plot",
    strip.text.y = element_text(angle = -90),
    strip.text = element_text(size = 18),
    panel.border = element_rect(colour = "grey80", fill = NA, linewidth = 1),
    legend.title = element_blank(),
    legend.position = "top"
  ))
  
opts_chunk$set(
  cache = FALSE,
  echo = FALSE
  )

baseline_git_commit_short <- substr(baseline_git_commit, 1, 7)
contender_git_commit_short <- substr(contender_git_commit, 1, 7)

source("R/functions.R")
```


```{r compute-runs}
#| include: false
#| results: 'asis'
#| cache: !expr '!is_gha()'

run_comp <- find_runs(baseline_git_commit, contender_git_commit, hardware_name)

if (length(run_comp) == 0) {
  knit_exit("No runs found for the given commits. Please check that the commits are correct and that the benchmark runs have completed.")
}

hardware_summary <- run_comp %>%
  mutate(commit.timestamp = ymd_hms(commit.timestamp)) %>%
  distinct(run_type, links.self, commit.sha, commit.timestamp, commit.url, hardware.cpu_model_name)
```


```{r message-missing-hardware}
#| results: 'asis'

if (nrow(hardware_summary) != 2L*length(hardware_name)) {
  ## fail early and informatively with when not using correct hardware
  missing_commits <- setdiff(c(contender_git_commit, baseline_git_commit), unique(hardware_summary$commit.sha)[1])
  cat(pluralize("Commits {missing_commits} w{?as/ere} not benchmarked with {glue_collapse(hardware_name, last = ' & ')} hardware."))
  knit_exit()
}
```



# Benchmark Run Summary
```{r table-run-summary}
hardware_summary %>% 
  mutate(
    commit.sha = glue("[{substr(commit.sha, 1, 7)}]({commit.url})"),
    run_type = glue("[{run_type}]({links.self})"),
    commit.timestamp = ymd_hms(commit.timestamp)
    ) %>% 
  select(run_type, commit.sha, commit.timestamp, hardware.cpu_model_name) %>% 
  arrange(hardware.cpu_model_name) %>% 
  gt() %>% 
  fmt_markdown(c("commit.sha", "run_type")) %>% 
  gt::cols_label(
    run_type = "Run Type",
    commit.sha = "Commit SHA",
    commit.timestamp = "Time of Commit",
    hardware.cpu_model_name = "Hardware"
  ) %>% 
  opt_table_font(font = google_font("Roboto Mono"))
```

```{r message-same-day-commit}
#| results: 'asis'

if (Sys.Date() %in% unique(as.Date(hardware_summary$commit.timestamp))) {
  ## inform early and informatively with when not using correct hardware
  cat("::: {.callout-warning}\n")
  cat("You are trying to examine commits that were made today. It is possible that not all benchmarks have been completed yet and plots below may show unexpected results.\n")
  cat(":::\n")
}
```

# Macrobenchmarks



```{r function-plots}
#| include: false

change_cols <- viridisLite::mako(2, begin = 0.5, end = 0.75)
names(change_cols) <- c(glue("{contender_git_commit_short} (contender) faster"), glue("{baseline_git_commit_short} (baseline) faster"))
```


```{r compute-process-macro-bm-data}
#| results: 'asis'
#| cache: !expr '!is_gha()'

# Compare the baseline to the contender for 
# macrobenchmarks
macro_bm_df <- run_comp %>%
  filter(hardware.name == "ursa-i9-9960x") %>%
  compare_baseline_to_contender() %>%
  filter(baseline$language %in% c("Python", "R"))

macro_bm_proced <- macro_bm_df %>%
  # csv-read had very different kinds of runs
  filter(baseline$benchmark_name != "csv-read") %>%
  # Remove the "TPCH-" prefix and any 0 at the start of the number
  mutate(baseline_tags = mutate(baseline$tags, query_id = gsub("^0", "", gsub("TPCH-", "", query_id)))) %>%
  mutate(dataset = case_when(
    baseline$benchmark_name == "tpch" ~ glue("Query id: {baseline_tags$query_id}"),
    baseline$benchmark_name == "wide-dataframe" ~ "wide-dataframe",
    baseline$benchmark_name == "partitioned-dataset-filter" ~ glue("{baseline$benchmark_name}: {baseline_tags$query} query"),
    TRUE ~ baseline_tags$dataset
  )) %>%
  group_by(baseline$language, baseline$benchmark_name) %>%
  group_map(~ tidy_compare(.x, .y))

macro_bm_list <- list()
for (i in seq_len(length(macro_bm_proced))) {
  one_conbench <- macro_bm_proced[[i]]
  lang <- unique(one_conbench$language)
  benchmark_name <- unique(one_conbench$benchmark_name)
  macro_bm_list[[lang]][[benchmark_name]] <- one_conbench
}

tpch_data <- macro_bm_list[["R"]][["tpch"]]

macro_bm_list[["R"]][which(names(macro_bm_list[["R"]]) %in% c("tpch"))] <- NULL
```

- [Conbench compare url](`r generate_compare_url(macro_bm_df)`)

## Benchmark Percent Changes

- Benchmarks are plotted using the percent change from baseline to contender. 
- Additional information on each benchmark is available by hovering over the relevant bar.

## Python
```{r plot-python}
#| echo: FALSE
#| results: 'asis'
#| warning: FALSE
 
#names(macro_bm_list)
lang <- "Python"
python_plots <- map_chr(names(macro_bm_list[[lang]]), \(bm_name) {
  knit_child(
    text = c(
      '```{r}',
      "#| results: 'asis'",
      'cat("###", bm_name, "\n")',
      'plot_comparison(macro_bm_list[[lang]][[bm_name]])',
      '```'
    ),
    envir = environment(),
    quiet = TRUE
  )
})

cat(python_plots, sep = "\n")
```


## R
```{r plot-r}
#| echo: FALSE
#| results: 'asis'
#| warning: FALSE
 
lang <- "R"
r_plots <- map_chr(names(macro_bm_list[[lang]]), \(bm_name) {
  knit_child(
    text = c(
      '```{r}',
      "#| results: 'asis'",
      'cat("###", bm_name, "\n")',
      'plot_comparison(macro_bm_list[[lang]][[bm_name]])',
      '```'
    ),
    envir = environment(),
    quiet = TRUE
  )
})

cat(r_plots, sep = "\n")
```

### tpch 
```{r plot-tpch}
#| echo: FALSE
#| results: 'asis'
#| warning: FALSE

tpch_p <- tpch_data %>%
  mutate(change_lab = change * 1.05) %>%
  mutate(query_id = as.numeric(query_id)) %>%
  ggplot() +
  aes(y = fct_reorder(dataset, query_id, .desc = TRUE), x = change, fill = pn_lab) +
  geom_col_interactive(
    aes(
      tooltip = glue(
        "Percent change: {round(change, 2)}%\n",
        "Difference: {difference}\n",
        "Dataset: {dataset}"
      )
    ),
    position = "dodge", alpha = 0.75
  ) +
  geom_text(aes(x = change_lab, hjust = ifelse(change >= 0, 0, 1), label = difference, colour = pn_lab), size = 7) +
  geom_vline(xintercept = 0, colour = "grey80") +
  facet_grid(rows = vars(format), cols = vars(scale_factor), labeller = label_both) +
  scale_fill_manual(values = change_cols) +
  scale_colour_manual(values = change_cols) +
  scale_x_continuous(expand = expansion(mult = 0.6)) +
  guides(colour = "none") +
  ylab("Dataset") +
  xlab("% change")

girafe(
  ggobj = tpch_p,
  options = list(
    opts_tooltip(use_fill = TRUE),
    opts_sizing(width = .7)
  ),
  width_svg = 16,
  height_svg = 20
)
```


# Microbenchmarks

```{r compute-process-micro-bm-data}
#| cache: !expr '!is_gha()'

micro_bm_df <- run_comp %>%
  filter(hardware.name == "ursa-thinkcentre-m75q") %>%
  compare_baseline_to_contender()

## unlike the plots above, this is added as single rectangular
## data.frame because that data structure is easier to work with
## using arquero
micro_bm_proced <- micro_bm_df %>%
  mutate(baseline_tags = baseline$tags) %>%
  group_by(baseline$language, baseline$benchmark_name) %>%
  group_modify(~ tidy_compare(.x, .y)) %>% 
  ungroup() %>% 
  filter(!is.na(name)) %>% 
  clean_names() %>% 
  select(language, baseline_benchmark_name, name, params, suite, analysis_pairwise_regression_indicated, analysis_pairwise_improvement_indicated, change, difference, pn_lab, analysis_lookback_z_score_z_score)
```

There are currently `r nrow(micro_bm_proced)` microbenchmarks in the Arrow benchmarks. The following comparisons are also available to be viewed in the [conbench ui](`r generate_compare_url(micro_bm_df)`). 

```{r}
micro_bm_proced %>% 
  count(language, analysis_pairwise_regression_indicated, analysis_pairwise_improvement_indicated) %>% 
  mutate(col_var = case_when(
    analysis_pairwise_regression_indicated == TRUE ~ "Regressions",
    analysis_pairwise_improvement_indicated == TRUE ~ "Improvements",
    TRUE ~ "Neither"
  )) %>%
  select(-all_of(starts_with("analysis"))) %>% 
  pivot_wider(names_from = col_var, values_from = n) %>% 
  rowwise() %>%
  mutate(Total = sum(c_across(c(Neither, Improvements, Regressions)))) %>%
  select(-Neither) %>%
  gt() %>% 
  cols_label(language = "Language") %>% 
  tab_spanner(
    label = "Number of microbenchmarks",
    columns = c(Improvements, Regressions, Total)
  ) %>%
  opt_table_font(font = google_font("Roboto Mono"))
```

Because of the large number of benchmarks, the top 20 benchmark results that deviate most from the baseline in both the positive and negative directions are presented below. 

::: {.callout-important collapse="true"}

## Top 20 negative z-scores

```{r}
top_zscore_table(micro_bm_proced, direction = "negative")
```


:::

::: {.callout-note collapse="true"}

## Top 20 positive z-scores

```{r}
top_zscore_table(micro_bm_proced, direction = "positive")
```

:::

## z-score distribution

Plotting the distribution of zscores for all benchmark results will help identify any systematic differences between the baseline and contender. 
 

```{ojs}
Plot.plot({
  y: {grid: true},
  x: {
    label: "z-score"
  },
  color: {legend: false},
  width: 1000,
  height: 400,
  marks: [
    Plot.rectY(microBmProced, Plot.binX({y: "count"}, {x: "analysis_lookback_z_score_z_score", fill: "suite", tip: true})),
    Plot.ruleY([0])
  ]
})
```

```{r ojs-defn}
ojs_define(ojs_micro_bm_proced = micro_bm_proced)
ojs_define(ojs_change_cols = rev(change_cols))
ojs_define(ojs_pn_lab = unique(micro_bm_proced$pn_lab))
```

```{ojs setup-micro-bm}
Plot = await import("https://esm.sh/@observablehq/plot");
import { aq, op } from '@uwdata/arquero';
boxWidth = 900
microBmProced = aq.from(transpose(ojs_micro_bm_proced))
```

## Microbenchmark explorer

Formal pairwise comparisons between the baseline and contender can further be broken down below by language, suite, and benchmark name. Additionally regressions and improvements can be filtered to show only those that are marked by conbench.


```{ojs filter-micro-bm}
// Top level: are there regressions/improvements?
viewof changes = Inputs.checkbox(["Regressions", "Improvements"], {
  label: md`**Benchmark Status**`,
  value: "Regressions"
  })

// Choose the state of the benchmark
microBmProcedChanges = {
  let microBmProcedParams;
  let hasRegressions = changes.includes("Regressions");
  let hasImprovements = changes.includes("Improvements");
  microBmProcedParams = microBmProced
      .params({hr: hasRegressions, hi: hasImprovements})
  if (hasRegressions && hasImprovements) {
    microBmProcedParams = microBmProced
      .filter((d, $) => d.analysis_pairwise_regression_indicated==$.hr || d.analysis_pairwise_improvement_indicated==$.hi);
  } else if (hasImprovements) {
    microBmProcedParams = microBmProced
      .filter((d, $) => d.analysis_pairwise_improvement_indicated==$.hi)
  } else if (hasRegressions) {
    microBmProcedParams = microBmProced
      .filter((d, $) => d.analysis_pairwise_regression_indicated==$.hr);
  } else {
    microBmProcedParams = microBmProced;
  }
  return microBmProcedParams;
}

// Choose the language
allLanguageValues = microBmProcedChanges.dedupe('language').array('language')

viewof languageSelected = Inputs.select(allLanguageValues, {
    label: md`**Language**`,
    value: [allLanguageValues[0]],
    width: boxWidth
})

// Choose the suite
allSuiteValues = microBmProcedChanges
    .filter(aq.escape(d => op.equal(d.language, languageSelected)))
    .dedupe('suite').array('suite')

viewof suiteSelected = Inputs.select(allSuiteValues, {
    label: md`**Suite**`,
    value: [allSuiteValues[0]],
    width: boxWidth
})

// Choose the benchmark
allNameValues = microBmProcedChanges
    .filter(aq.escape(d => op.equal(d.language, languageSelected)))
    .filter(aq.escape(d => op.equal(d.suite, suiteSelected)))
    .dedupe('name').array('name')

viewof nameSelected = Inputs.select(allNameValues, {
    label: md`**Name**`,
    value: [allNameValues[0]],
    width: boxWidth
})

microBmProcedChangesFiltered = microBmProcedChanges
    .filter(aq.escape(d => op.equal(d.language, languageSelected)))
    .filter(aq.escape(d => op.equal(d.suite, suiteSelected)))
    .filter(aq.escape(d => op.equal(d.name, nameSelected)))
```

```{ojs plot-micro-bm}
// long labels need special handlings
margins = {
  let hasRegressions = changes.includes("Regressions");
  let hasImprovements = changes.includes("Improvements");
  let margin = [300, 300];
  if (hasRegressions && hasImprovements) {
    margin = margin;
  } else if (hasImprovements) {
    margin = [0, 600];
  } else if (hasRegressions) {
    margin = [600, 0];
  } 
  return margin;
}

Plot.plot({
  width: 1200,
  height: (microBmProcedChangesFiltered.numRows()*30)+100, //adjust height of plot based on number of rows
  marginRight: margins[0],
  marginLeft: margins[1],
  label: null,
  x: {
    axis: "top",
    label: "% change",
    labelAnchor: "center",
    percent: true,
    labelOffset: 30
  },
  style: {
    fontSize: "14px",
    fontFamily: "Roboto Mono"
    },
  color: {
    range: ojs_change_cols,
    domain: ojs_pn_lab,
    type: "categorical",
    legend: true
  },
  marks: [
    Plot.barX(microBmProcedChangesFiltered, {
      y: "params", 
      x: "change", 
      color: "black",
      fill: "pn_lab", 
      fillOpacity: 0.75,
      sort: {y: "x"},
      channels: {difference: "difference", params: "params"}, 
      tip: true
      }),
    Plot.gridX({stroke: "white", strokeOpacity: 0.5}),
    Plot.ruleX([0]),
    d3
      .groups(microBmProcedChangesFiltered, (d) => d.change > 0)
      .map(([posneg, dat]) => [
        Plot.axisY({
          x: 0,
          ticks: dat.map((d) => d.params),
          tickSize: 0,
          anchor: posneg ? "left" : "right"
        }),
      ])
  ]
})
```


```{css, echo=FALSE}
div.main-container {
  max-width: 2000px;
}
```
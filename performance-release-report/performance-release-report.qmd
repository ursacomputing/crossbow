---
title: "Arrow Release Benchmark Report"
execute: 
  warning: false
  echo: false
format: 
  html:
    grid:
        sidebar-width: 0px
        body-width: 2000px
        margin-width: 340px
        gutter-width: 1.5rem
    self-contained: true
    page-layout: full
    toc: true
    toc-title: "Contents"
    toc-depth: 3
    margin-left: 30px
    link-external-newwindow: true
    theme: cosmo
---


```{r setup}
#| include: false

Sys.unsetenv("CONBENCH_PASSWORD")


baseline_git_commit <- Sys.getenv("BASELINE_GIT_COMMIT") 
contender_git_commit <-  Sys.getenv("CONTENDER_GIT_COMMIT")
# baseline_git_commit <- '5bf86ab4d9e9bc5bb7e1c6e65a55d9f1723597bf'
# contender_git_commit <-  'b7d2f7ffca66c868bd2fce5b3749c6caa002a7f0'
hardware_name <- c("ursa-i9-9960x", "ursa-thinkcentre-m75q")

library(dplyr)
library(ggplot2)
library(tidyr)
library(glue)
library(purrr)
library(conbenchcoms)
library(cli)
library(lubridate)
library(knitr)
library(gt)
library(forcats)
library(showtext)
library(ggiraph)
library(janitor)

## add fonts for ggplot2
## TODO: add for tooltip font
font_add_google('Roboto Mono', 'roboto')
showtext_auto()

fig_width <- 16
fig_height <- 7

## tweaks to theme
theme_set(theme_minimal(base_size = 24, base_family = "roboto") %+replace%
  theme(
    plot.title.position = "plot",
    strip.text.y = element_text(angle = -90),
    strip.text = element_text(size = 18),
    panel.border = element_rect(colour = "grey80", fill = NA, linewidth = 1),
    legend.title = element_blank(),
    legend.position = "top"
  ))
  
opts_chunk$set(
  cache = FALSE,
  echo = FALSE
  )

baseline_git_commit_short <- substr(baseline_git_commit, 1, 7)
contender_git_commit_short <- substr(contender_git_commit, 1, 7)

source("R/functions.R")
```

```{r warn-gha}
if (!is_gha()) {
  message("When run locally, this report will cache results to avoid re-running benchmarks. 
To see the most up-to-date results, please run this report on GitHub Actions or delete the cache.")
}
```


```{r warn-commits-not-set}
if (!nzchar(baseline_git_commit) | !nzchar(contender_git_commit)) {
  knit_exit("Please set the BASELINE_GIT_COMMIT & CONTENDER_GIT_COMMIT environment variables to the commit hash of the baseline and the release.")
}
```

```{r compute-conbench-requests}
#| include: false
#| results: 'asis'
#| cache: !expr '!is_gha()'

run_comp <- find_runs(baseline_git_commit, contender_git_commit, hardware_name)

if (length(run_comp) == 0) {
  knit_exit("No runs found for the given commits. Please check that the commits are correct and that the benchmark runs have completed.")
}

# Compare the baseline to the contender for 
# macrobenchmarks
ursa_i9_bm <- run_comp %>%
  filter(hardware.name == "ursa-i9-9960x") %>%
  compare_baseline_to_contender()

macro_bm_df <- ursa_i9_bm %>%
  filter(baseline$language %in% c("Python", "R")) 

# microbenchmarks
micro_bm_df <- run_comp %>%
  filter(hardware.name == "ursa-thinkcentre-m75q") %>%
  compare_baseline_to_contender() %>% 
  bind_rows(ursa_i9_bm %>% filter(baseline$language %in% "JavaScript"))
```


```{r message-missing-hardware}
#| results: 'asis'

## extract hardware information
hardware_summary <- run_comp %>%
  mutate(commit.timestamp = ymd_hms(commit.timestamp)) %>%
  mutate(links.self = glue("https://conbench.ursa.dev/runs/{id}")) %>% 
  distinct(run_type, links.self, commit.sha, commit.timestamp, commit.url, hardware.cpu_model_name, id)

if (nrow(hardware_summary) != 2L*length(hardware_name)) {
  ## fail early and informatively with when not using correct hardware
  missing_commits <- setdiff(c(contender_git_commit, baseline_git_commit), unique(hardware_summary$commit.sha)[1])
  cat(pluralize("Commits {missing_commits} w{?as/ere} not benchmarked with {glue_collapse(hardware_name, last = ' & ')} hardware."))
  knit_exit()
}
```



# Benchmark Run Summary
```{r table-run-summary}

## get additional information about the benchmarks
benchmark_summary <- map_df(list(macro_bm_df, micro_bm_df), get_language_summary_from_comparison)


run_summary <- hardware_summary %>%
  left_join( ## join on run_id
    benchmark_summary,
    by = c("id" = "run_id")
  ) %>%
  mutate(
    commit.sha = glue("[{substr(commit.sha, 1, 7)}]({commit.url})"),
    run_type = glue("[{run_type}]({links.self})"),
    commit.timestamp = ymd_hms(commit.timestamp)
  ) %>%
  select(run_type, commit.sha, commit.timestamp, hardware.cpu_model_name, languages, benchmark_type, n_benchmarks) %>%
  arrange(benchmark_type, hardware.cpu_model_name)

run_summary %>% 
  gt() %>% 
  fmt_markdown(c("commit.sha", "run_type", "benchmark_type")) %>% 
  gt::cols_label(
    run_type = "Run Type",
    commit.sha = "Commit SHA",
    commit.timestamp = "Time of Commit",
    hardware.cpu_model_name = "Hardware",
    languages = "Languages",
    benchmark_type = "Benchmark Type",
    n_benchmarks = "Number of Benchmarks"
  ) %>% 
  tab_footnote(
    "When we compare benchmark results, we always have a contender (the new code that we are considering) and a baseline (the old code that were are comparing to). The historic distribution will be drawn from all benchmark results on commits in the baseline commit's git ancestry, up to and including all runs on the baseline commit itself. In this context, a baseline is typically the last Arrow release and the contender is the current release candidate.",
    locations = cells_column_labels(columns = "run_type")
  ) %>% 
  opt_table_font(font = google_font("Roboto Mono"))
```

```{r message-same-day-commit}
#| results: 'asis'

if (Sys.Date() %in% unique(as.Date(hardware_summary$commit.timestamp))) {
  ## inform early and informatively with when not using correct hardware
  cat("::: {.callout-warning}\n")
  cat("You are trying to examine commits that were made today. It is possible that not all benchmarks have been completed yet and plots below may show unexpected results.\n")
  cat(":::\n")
}
```

# Macrobenchmarks {#macro-bm}



```{r function-plots}
#| include: false

change_cols <- viridisLite::mako(2, begin = 0.5, end = 0.75)
names(change_cols) <- c(glue("{contender_git_commit_short} (contender) faster"), glue("{baseline_git_commit_short} (baseline) faster"))
```


```{r compute-process-macro-bm-data}
#| results: 'asis'

macro_bm_proced <- macro_bm_df %>%
  # csv-read had very different kinds of runs
  filter(baseline$benchmark_name != "csv-read") %>%
  # Remove the "TPCH-" prefix and any 0 at the start of the number
  mutate(baseline_tags = mutate(baseline$tags, query_id = gsub("^0", "", gsub("TPCH-", "", query_id)))) %>%
  mutate(dataset = case_when(
    baseline$benchmark_name == "tpch" ~ glue("Query id: {baseline_tags$query_id}"),
    baseline$benchmark_name == "wide-dataframe" ~ "wide-dataframe",
    baseline$benchmark_name == "partitioned-dataset-filter" ~ glue("{baseline$benchmark_name}: {baseline_tags$query} query"),
    TRUE ~ baseline_tags$dataset
  )) %>%
  group_by(baseline$language, baseline$benchmark_name) %>%
  group_map(~ tidy_compare(.x, .y))

macro_bm_list <- list()
for (i in seq_len(length(macro_bm_proced))) {
  one_conbench <- macro_bm_proced[[i]]
  lang <- unique(one_conbench$language)
  benchmark_name <- unique(one_conbench$benchmark_name)
  macro_bm_list[[lang]][[benchmark_name]] <- one_conbench
}

tpch_data <- macro_bm_list[["R"]][["tpch"]]

macro_bm_list[["R"]][which(names(macro_bm_list[["R"]]) %in% c("tpch"))] <- NULL
```

Live Conbench UI views for the macrobenchmarks are available at this [url](`r generate_compare_url(macro_bm_df)`). Conbench is an additional method to explore the results of the benchmarks particularly if you want to see results from more of the history or see more metadata. 

## Benchmark Percent Changes

- Benchmarks are plotted using the percent change from baseline to contender. 
- Additional information on each benchmark is available by hovering over the relevant bar.

## Python
```{r plot-python}
#| echo: FALSE
#| results: 'asis'
#| warning: FALSE
 
#names(macro_bm_list)
lang <- "Python"

bm_names <- names(macro_bm_list[[lang]])
bm_names <- bm_names[!bm_names %in% c("dataset-serialize")] ## dropping dataset-serialize for now

python_plots <- map_chr(bm_names, \(bm_name) {
  knit_child(
    text = c(
      glue("```{r plot-<<lang>>-<<bm_name>>}", .open = "<<", .close = ">>"),
      "#| results: 'asis'",
      'cat("###", bm_name, "\n")',
      "plot_comparison(macro_bm_list[[lang]][[bm_name]])",
      "```"
    ),
    envir = environment(),
    quiet = TRUE
  )
})

cat(python_plots, sep = "\n")
```


## R
```{r plot-r}
#| echo: FALSE
#| results: 'asis'
#| warning: FALSE
 
lang <- "R"
r_plots <- map_chr(names(macro_bm_list[[lang]]), \(bm_name) {
  knit_child(
    text = c(
      glue("```{r plot-<<lang>>-<<bm_name>>}", .open = "<<", .close = ">>"),
      "#| results: 'asis'",
      'cat("###", bm_name, "\n")',
      "plot_comparison(macro_bm_list[[lang]][[bm_name]])",
      "```"
    ),
    envir = environment(),
    quiet = TRUE
  )
})

cat(r_plots, sep = "\n")
```

### tpch 
```{r plot-tpch}
#| echo: FALSE
#| results: 'asis'
#| warning: FALSE

tpch_p <- tpch_data %>%
  mutate(change_lab = change * 1.05) %>%
  mutate(query_id = as.numeric(query_id)) %>%
  ggplot() +
  aes(y = fct_reorder(dataset, query_id, .desc = TRUE), x = change, fill = pn_lab) +
  geom_col_interactive(
    aes(
      tooltip = glue(
        "Percent change: {round(change, 2)}%\n",
        "Difference: {difference}\n",
        "Dataset: {dataset}"
      )
    ),
    position = "dodge", alpha = 0.75
  ) +
  geom_text(aes(x = change_lab, hjust = ifelse(change >= 0, 0, 1), label = difference, colour = pn_lab), size = 7) +
  geom_vline(xintercept = 0, colour = "grey80") +
  facet_grid(rows = vars(format), cols = vars(scale_factor), labeller = label_both) +
  scale_fill_manual(values = change_cols) +
  scale_colour_manual(values = change_cols) +
  scale_x_continuous(expand = expansion(mult = 0.6)) +
  guides(colour = "none") +
  ylab("Dataset") +
  xlab("% change")

girafe(
  ggobj = tpch_p,
  options = list(
    opts_tooltip(use_fill = TRUE),
    opts_sizing(width = .7)
  ),
  width_svg = 16,
  height_svg = 20
)
```


# Microbenchmarks {#micro-bm}

```{r compute-process-micro-bm-data}


## unlike the plots above, this is added as single rectangular
## data.frame because that data structure is easier to work with
## using arquero
micro_bm_proced <- micro_bm_df %>%
  mutate(baseline_tags = baseline$tags) %>%
  group_by(baseline$language, baseline$benchmark_name) %>%
  group_modify(~ tidy_compare(.x, .y)) %>% 
  ungroup() %>% 
  filter(!is.na(name)) %>% 
  filter(!is.na(analysis.lookback_z_score.regression_indicated)) %>% ## indicator of some empty data
  ## this will enable the yaxis to be populated with names when params is NA. params is preferable because it is more specific
  mutate(params = ifelse(is.na(params), baseline.case_permutation, params)) %>% 
  rowwise() %>%
  mutate(params = paste(strwrap(params, 10), collapse="\n")) %>% 
  clean_names() %>% 
  select(language, baseline_benchmark_name, name, params, suite, analysis_pairwise_regression_indicated, analysis_pairwise_improvement_indicated, change, difference, pn_lab, analysis_lookback_z_score_z_score, analysis_lookback_z_score_z_threshold, analysis_pairwise_percent_change, baseline_single_value_summary, contender_single_value_summary, cb_url, unit)
```

There are currently `r nrow(micro_bm_proced)` microbenchmarks in the Arrow benchmarks. The following comparisons are also available to be viewed in the [Conbench UI](`r generate_compare_url(micro_bm_df)`). 

```{r table-micro-bm-summary}
threshold <- unique(micro_bm_proced$analysis_lookback_z_score_z_threshold)
threshold <- threshold[!is.na(threshold)]
micro_bm_proced %>% 
  count(language, analysis_pairwise_regression_indicated, analysis_pairwise_improvement_indicated) %>% 
  mutate(col_var = case_when(
    analysis_pairwise_regression_indicated == TRUE ~ "Regressions",
    analysis_pairwise_improvement_indicated == TRUE ~ "Improvements",
    TRUE ~ "Stable"
  )) %>%
  select(-all_of(starts_with("analysis_pairwise"))) %>% 
  pivot_wider(names_from = col_var, values_from = n) %>% 
  rowwise() %>%
  mutate(Total = sum(c_across(c(Stable, Improvements, Regressions)))) %>%
  mutate(`z-score threshold` = threshold, .after = language) %>% 
  gt() %>% 
  cols_label(language = "Language") %>% 
  tab_spanner(
    label = "Number of microbenchmarks",
    columns = c(Stable, Improvements, Regressions, Total)
  ) %>%
  opt_table_font(font = google_font("Roboto Mono"))
```

Because of the large number of benchmarks, the top 20 benchmark results that deviate most from the baseline in both the positive and negative directions are presented below. _All_ microbenchmark results for this comparison can be explored interactively in the [microbenchmark explorer](#micro-bm-explorer).

::: {.callout-important collapse="true"}

## Largest 20 regressions between baseline and contender

```{r table-top-zscores-negative}
top_zscore_table(micro_bm_proced, direction = "regression")
```


:::

::: {.callout-note collapse="true"}

## Largest 20 improvements between baseline and contender

```{r table-top-zscores-positive}
top_zscore_table(micro_bm_proced, direction = "improvement")
```

:::

## z-score distribution

Plotting the distribution of zscores for all microbenchmark results will help identify any systematic differences between the baseline and contender. The shape of the distribution of z-scores provides a sense of the overall performance of the contender relative to the baseline. Narrow distirbutions centered around 0 indicate that the contender is performing similarly to the baseline. Wider distributions indicate that the contender is performing differently than the baseline with left skewing indicating regressions and right skewing indicating improvements.
 

```{ojs}
Plot.plot({
  y: {grid: true},
  x: {
    label: "z-score"
  },
  color: {legend: false},
  width: 1000,
  height: 400,
  marks: [
    Plot.rectY(microBmProced, Plot.binX({y: "count"}, {x: "analysis_lookback_z_score_z_score", fill: "grey", tip: true})),
    Plot.ruleY([0])
  ]
})
```

```{r ojs-defn}
ojs_define(ojs_micro_bm_proced = micro_bm_proced)
ojs_define(ojs_change_cols = rev(change_cols))
ojs_define(ojs_pn_lab = unique(micro_bm_proced$pn_lab))
```

```{ojs setup-micro-bm}
Plot = await import("https://esm.sh/@observablehq/plot");
import { aq, op } from '@uwdata/arquero';
boxWidth = 900
microBmProced = aq.from(transpose(ojs_micro_bm_proced))
```

## Microbenchmark explorer {#micro-bm-explorer}

This microbenchmarks explorer allows you to filter the microbenchmark results by language, suite, and benchmark name and toggle regressions and improvements based on a threshold level of `r threshold` z-scores. Languages, suite and benchmark name default to showing all results for that category. Additional benchmark parameters are displayed on the vertical axis resulting in each bar representing a case permutation. If a becnhmark does not have additional parameters, the full case permutation string is displayted. The display can be further filtered by selecting a specific language, suite, or benchmark name. Each bar can be clicked to open the Conbench UI page for that benchmark providing additional history and metadata for that case permutation.

```{ojs filter-micro-bm}
// Top level: are there regressions/improvements?
viewof changes = Inputs.checkbox(["Regressions", "Improvements"], {
  label: md`**Benchmark Status**`,
  value: ["Regressions"]
  })

// Choose the state of the benchmark
microBmProcedChanges = {
  let microBmProcedParams;
  let hasRegressions = changes.includes("Regressions");
  let hasImprovements = changes.includes("Improvements");
  microBmProcedParams = microBmProced
      .params({hr: hasRegressions, hi: hasImprovements})
  if (hasRegressions && hasImprovements) {
    microBmProcedParams = microBmProced
      .filter((d, $) => d.analysis_pairwise_regression_indicated==$.hr || d.analysis_pairwise_improvement_indicated==$.hi);
  } else if (hasImprovements) {
    microBmProcedParams = microBmProced
      .filter((d, $) => d.analysis_pairwise_improvement_indicated==$.hi)
  } else if (hasRegressions) {
    microBmProcedParams = microBmProced
      .filter((d, $) => d.analysis_pairwise_regression_indicated==$.hr);
  } else {
    microBmProcedParams = microBmProced;
  }
  return microBmProcedParams;
}

// Choose the language
allLanguageValues = ["All languages"].concat(microBmProcedChanges.dedupe('language').array('language'))

viewof languageSelected = Inputs.select(allLanguageValues, {
    label: md`**Language**`,
    value: [allLanguageValues[0]],
    width: boxWidth
})

languages = {
  return (languageSelected === "All languages")
  ? microBmProcedChanges // If languageSelected is "All languages", no filtering is applied
  : microBmProcedChanges.filter(aq.escape(d => op.includes(d.language, languageSelected)));
}


allSuiteValues = ["All suites"].concat(languages.dedupe('suite').array('suite'))

// Choose the suite
viewof suiteSelected = Inputs.select(allSuiteValues, {
    label: md`**Suite**`,
    value: [allSuiteValues[0]],
    width: boxWidth
})


suites = {
  return (suiteSelected === "All suites")
  ? languages 
  : languages.filter(aq.escape(d => op.includes(d.suite, suiteSelected)));
}

allNameValues = ["All benchmarks"].concat(suites.dedupe('name').array('name'))

// Choose the benchmark
viewof nameSelected = Inputs.select(allNameValues, {
    label: md`**Benchmark Name**`,
    value: [allNameValues[0]],
    width: boxWidth
})

microBmProcedChangesFiltered = {
  return (nameSelected === "All benchmarks")
  ? suites 
  : suites.filter(aq.escape(d => op.includes(d.name, nameSelected)));
}
```

```{ojs plot-micro-bm}
// long labels need special handlings
margins = {
  let hasRegressions = changes.includes("Regressions");
  let hasImprovements = changes.includes("Improvements");
  let margin = [300, 300];
  if (hasRegressions && hasImprovements) {
    margin = margin;
  } else if (hasImprovements) {
    margin = [0, 600];
  } else if (hasRegressions) {
    margin = [600, 0];
  } 
  return margin;
}

Plot.plot({
  width: 1200,
  height: (microBmProcedChangesFiltered.numRows()*30)+100, //adjust height of plot based on number of rows
  marginRight: margins[0],
  marginLeft: margins[1],
  label: null,
  x: {
    axis: "top",
    label: "% change",
    labelAnchor: "center",
    labelOffset: 30
  },
  style: {
    fontSize: "14px",
    fontFamily: "Roboto Mono"
    },
  color: {
    range: ojs_change_cols,
    domain: ojs_pn_lab,
    type: "categorical",
    legend: true
  },
  marks: [
    Plot.barX(microBmProcedChangesFiltered, {
      y: "params", 
      x: "change", 
      color: "black",
      fill: "pn_lab", 
      fillOpacity: 0.75,
      sort: {y: "x"},
      channels: {difference: "difference", params: "params"}, 
      href: "cb_url",
      tip: true
      }),
    Plot.gridX({stroke: "white", strokeOpacity: 0.5}),
    Plot.ruleX([0]),
    d3
      .groups(microBmProcedChangesFiltered, (d) => d.change > 0)
      .map(([posneg, dat]) => [
        Plot.axisY({
          x: 0,
          ticks: dat.map((d) => d.params),
          tickSize: 0,
          anchor: posneg ? "left" : "right"
        }),
      ])
  ]
})
```


```{css, echo=FALSE}
div.main-container {
  max-width: 2000px;
}
```